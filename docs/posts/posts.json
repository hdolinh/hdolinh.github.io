[
  {
    "path": "posts/2021-11-14-stats-final/",
    "title": "1990-2021 data suggests well depth rate is increasing in California",
    "description": "",
    "author": [
      {
        "name": "Halina Do-Linh",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [],
    "contents": "\n\nContents\nQuestion\nAre we drilling deeper wells faster over time in California?\n\nData\nData Histograms\n\nAnalysis Plan\nResults\nMultiple Linear Regression\nMulitple Linear Regression Model with Interaction Effect\nTime Series Analysis - Decomposition\n\nFuture Research\nAppendix\nFigures\nReferences\n\n\n\n\nShow code\n\nwells <- read_csv(here::here(\"_posts\", \"2021-11-14-stats-final\",\"data\", \"08_CA_AK.csv\"))\n\n\n\nQuestion\nAre we drilling deeper wells faster over time in California?\n1.5 million people in California rely on domestic well water1. Majority of the 1.5 million reside in rural areas without alternative drinking-water sources. Furthermore, in the Central Valley, it has been found that one in five wells now run dry due to groundwater level declines2. While some have responded to this issue by drilling deeper, this practice has also been found to be unsustainable because it is costly, it is impractical in certain hydrogeological conditions, and deep groundwater is often brackish or saline3. While we are aware that groundwater levels are declining and that deeper drilling is unsustainable, we do not know how fast we are drilling deeper in California over time.\nIt is important to study groundwater trends because water is an important resource in California and it is crucial for groundwater management to be sustainable and equitable. By understanding the rate at which we are drilling deeper wells over time, we can improve groundwater management and make well water a more secure resource, especially for communities that depend on it.\nData\nMy data was obtained by Dr. Debra Perrone and Dr. Scott Jasechko for their research on groundwater wells. California domestic well water data was obtained through personal communication with state representatives from the California Department of Water Resources (CADWR) in March 2016 and April 2016. The CADWR maintains the California Groundwater Well Completion Report database. Dr. Perrone and Dr. Jasechko reported that the data may contain biases by region, time, purpose, and compliance.\nThis data is international, but I subsetted the data to only contain wells from California. I also added region codes based on the California Complete Count Office, which groups California’s 58 counties into 10 regions based on hard-to-count populations, like-mindedness of the counties, capacity of community-based organizations within the counties, and state Census staff workload capabilities.\n\n\nShow code\n\nwells_tidy <- wells %>% \n  janitor::clean_names() %>% \n  select(\"well_id\", \"county\", \"purpose_5\", \"date_completed\", \"depth_m\") %>%\n  filter(str_detect(well_id, pattern = \"CA\")) %>% \n  filter(depth_m > -1) %>% \n  mutate(date_completed = lubridate::parse_date_time(date_completed, orders = c(\"mdy\", \"mdy HMS\", \"mdY\"))) %>% \n  mutate(year_completed = lubridate::year(date_completed)) %>% \n  filter(year_completed >= 1900 & year_completed <= 2021)\n\n\n\n\n\nShow code\n\nregion1 <- c(\"Butte\", \"Colusa\", \"El Dorado\", \"Glenn\", \"Lassen\", \"Modoc\", \"Nevada\", \"Placer\", \"Plumas\", \"Sacramento\", \"Shasta\", \"Sierra\", \"Siskiyou\", \"Sutter\", \"Tehama\", \"Yolo\", \"Yuba\")\nregion1_code <- 1\n\nregion2 <- c(\"Del Norte\", \"Humboldt\", \"Lake\", \"Mendocino\", \"Napa\", \"Sonoma\", \"Trinity\")\nregion2_code <- 2\n\nregion3 <- c(\"Alameda\", \"Contra Costa\", \"Marin\", \"San Francisco\", \"San Mateo\", \"Santa Clara\", \"Solano\")\nregion3_code <- 3\n\nregion4 <- c(\"Alpine\", \"Amador\", \"Calaveras\", \"Madera\", \"Mariposa\", \"Merced\", \"Mono\", \"San Joaquin\", \"Stanislaus\", \"Tuolumne\")\nregion4_code <- 4\n\nregion5 <- c(\"Monterey\", \"San Benito\", \"San Luis Obispo\", \"Santa Barbara\", \"Santa Cruz\", \"Ventura\")\nregion5_code <- 5\n\nregion6 <- c(\"Fresno\", \"Inyo\", \"Kern\", \"Kings\", \"Tulare\")\nregion6_code <- 6\n\nregion7 <- c(\"Riverside\", \"San Bernardino\")\nregion7_code <- 7\n\nregion8 <- \"Los Angeles\"\nregion8_code <- 8\n\nregion9 <- \"Orange\"\nregion9_code <- 9\n\nregion10<- c(\"Imperial\", \"San Diego\")\nregion10_code <- 10\n\nwells_region <- wells_tidy %>% \n  mutate(region_code = case_when(county %in% region1 ~ region1_code,\n                                 county %in% region2 ~ region2_code,\n                                 county %in% region3 ~ region3_code,\n                                 county %in% region4 ~ region4_code,\n                                 county %in% region5 ~ region5_code,\n                                 county %in% region6 ~ region6_code,\n                                 county %in% region7 ~ region7_code,\n                                 county %in% region8 ~ region8_code,\n                                 county %in% region9 ~ region9_code,\n                                 county %in% region10 ~ region10_code,\n                                 TRUE ~ 0)) %>% \n  mutate(region_code = as.factor(region_code))\n\n\n\n\n\nShow code\n\nnorcal <- c(1, 2, 3, 4, 5)\nnorcal_code <- \"north\"\n\nsocal <- c(6, 7, 8, 9, 10)\nsocal_code <- \"south\"\n\nregion_north_south <- wells_region %>% \n  mutate(north_south = case_when(region_code %in% norcal ~ norcal_code,\n                                 region_code %in% socal ~ socal_code,\n                                 TRUE ~ \"NA\")) %>%\n  mutate(north_south = as.factor(north_south)) %>% # make north_south a factor variable\n  group_by(year_completed, north_south) %>% \n  mutate(mean_depth_m = mean(depth_m)) # yearly averages by region\n\n\n\nData Histograms\nAfter taking the natural log of well depth (m), it appears to have a normal distribution . It also looks like majority of wells are located in Region 1 or Superior California (Figure 1).\n\n\nShow code\n\nlog_depth_m <- log(wells_region$depth_m)\n\ndepth_hist <- ggplot(data = wells_region, aes(log_depth_m)) +\n  geom_histogram(fill = \"light gray\") +\n  labs(x = \"Log of Well Depth (m)\",\n       y = \"Count\",\n       title = \"Well depths (m) in California Transformed by Natural Log (1900-2021)\") +\n  theme_light() +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(size = 9))\n\n\n\n\n\nShow code\n\n# to color cannot use \"color\" in histogram, need to use \"fill\"\nregion_bar <- ggplot(data = wells_region, aes(x = region_code)) + \n  geom_bar(fill = \"light gray\") +\n  labs(x = \"Region Code\",\n       y = \"Count\",\n       title = \"Distribution of wells by region (1900-2021)\") +\n  theme_light() +\n  theme(panel.grid = element_blank(),\n        plot.title = element_text(size = 9))\n\n\n\n\n\nShow code\n\ndepth_hist / region_bar\n\n\n\n\nFigure 1: Well depth (m) data transformed by natural log were normally distributed (top). Majority of wells in California can be found in Northern regions of California identified here as region codes 1-5 (bottom).\n\n\n\nExploratory graphs of total counts of wells both over time and by region suggests that overall we are completing less wells (Figure 1). However, we are also seeing that we could be potentially drilling deeper wells over time (Figure 2).\n\n\nShow code\n\nwells_depth_year <- wells_region %>% \n  select(well_id, year_completed, depth_m) %>% \n  group_by(year_completed) %>% \n  summarize(mean_depth_m_year = mean(depth_m))\n\naverage_yr_well_depth_plot <-ggplot(data = wells_depth_year, aes(x = year_completed, y = mean_depth_m_year)) +\n  geom_line() +\n  labs(x = \"Year well was completed\",\n       y = \"Well depth (m)\",\n       title = \"Yearly Average Well Depth (m) from 1990 to 2021\") +\n  theme_light() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nShow code\n\nwells_counts_year <- wells_region %>% \n  group_by(year_completed) %>% \n  count() %>% \n  rename(\"total_year\" = n)\n\ntot_wells_yr <- ggplot(data = wells_counts_year, aes(x = year_completed, y = total_year)) +\n  geom_line() +\n  labs(x = \"Year well was completed\",\n       y = \"Count\",\n       title = \"Total number of wells per year\") +\n  theme_light() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nShow code\n\ntot_wells_yr / average_yr_well_depth_plot\n\n\n\n\nFigure 2: Number of wells are appearing to decrease overtime in California with a significant drop in 2005 (top). Average well depth (m) are appearing to increase overtime in California with a significant increase after 2010 (bottom).\n\n\n\nAnalysis Plan\n1. Regression Analysis\nUse a multiple linear regression model to test if there is an effect of time and region on well depth (m), and if there is an effect - find what is the rate of well depth (m) over time. \\[welldepth(m) =\\beta_{0}+\\beta_{1} \\cdot year_i +\\beta_{2} \\cdot region_i+\\varepsilon_i\\]\nI added an interaction effect to my multiple linear regression model to see if the effect of year on well depth (m) depends on region. I predict there may be deeper wells in the northern regions than the southern regions since majority of California’s drinking water comes from northern regions, and that the rate of well depth will be faster in the northern region than the southern region. \\[welldepth(m) =\\beta_{0}+\\beta_{1} \\cdot year_i +\\beta_{2} \\cdot region_i+ \\beta_{3} \\cdot year_i \\cdot region_i + \\varepsilon_i\\]\n2. Time Series Analysis\nRun decomposition analysis to test for seasonality. From my exploratory graphs, I visually saw some evidence of cyclical patterns (see Figure 6 in Appendix) and wanted to test to see if this was significant.\nResults\nMultiple Linear Regression\n\n\nShow code\n\nmod <- lm(mean_depth_m ~ year_completed + north_south, data = region_north_south)\n\nparallel_slopes <- ggplot(data = region_north_south, aes(x = year_completed, y = mean_depth_m,  color = north_south)) +\n  geom_point() +\n  geom_line(data = augment(mod), aes(y = .fitted, color = north_south)) +\n  labs(x = \"Year well was completed\",\n       y = \"Yearly average well depth (m)\") +\n  scale_colour_discrete(\"North South Region\") +\n  theme_light() +\n  theme(panel.grid = element_blank())\nparallel_slopes\n\n\n\n\nFigure 3: Mulitple linear regression model of year and region on well depth. The regression is significant with a p-value < 0.00000000000000022 at the 0.01 significance level. Adjusted R-squared value 0.7354648.\n\n\n\nReporting & Interpretation of Coefficients: The intercept coefficient tells us that we expect to see a well depth of -292.98 m at year 0 in the south region (this is nonsensical). The year_completed coefficient tells us that we expect well depth to increase by 0.178 m each year, holding region constant. The north_south coefficient tells us that we expect to see, on average, a 23.85 m increase in well depth in the south region compared to the north region, holding year constant.\nOverall Interpretation: This parallel slopes model tells us that year, regardless of region, will have the same impact on yearly average well depth (m). This impact is significant with a p-value of 0.00000000000000022 at the significance level 0.01. The adjusted r-squared also reports that 74% of the data is explained by year. So over time we are seeing that year is significantly increasing well depth (m) by 0.178 m every year. This is a fairly fast rate.\nMulitple Linear Regression Model with Interaction Effect\n\n\nShow code\n\nmod_interaction <- lm(mean_depth_m ~ year_completed + north_south + year_completed:north_south, data = region_north_south) \n\ninteraction_plot <- region_north_south %>% \n  ggplot(aes(x = year_completed, y = mean_depth_m, color = north_south)) +\n  geom_point() +\n  geom_line(data = augment(mod_interaction), aes(y = .fitted, color = north_south)) +\n  labs(x = \"Year well was completed\",\n       y = \"Yearly average well depth (m)\") +\n  scale_colour_discrete(\"North South Region\") +\n  theme_light() +\n  theme(panel.grid = element_blank())\ninteraction_plot\n\n\n\n\nFigure 4: Interaction effect of year and region on well depth. The interaction effect is significant with a p-value < 0.00000000000000022 at the 0.01 significance level. Adjusted R-squared value is 0.7563347.\n\n\n\nReporting & Interpretation of Coefficients: The intercept coefficient tells us that we expect to see a well depth of -493.84 m at year 0 in the north region (this is nonsensical). The year_completed coefficient tells us that we expect well depth to increase by 0.27 m each year for a well located in the northern region. The north_south coefficient tells us that well depths are, on average, 689.50 m deeper in the north region than the south region at year 0 (this is also nonsensical). The year_completed:north_south coefficient tells us that the change in average well depth (m) after one year for a well in a northern region compared to a well in the southern region is -0.334 m.\nOverall Interpretation: This interaction effect tells us that the relationship between year and well_depth varies with region. This is a significant effect with a p-value of 0.00000000000000022 at the the significance level 0.01. The adjusted r-squared is reports that 75% of the data is explained by this interaction effect. Since this is a slight increase from the adjusted \\(R^2\\) value from the model with no interaction, this tells us that the interaction model improves the model fit (at least a little). The rate of the wells being drilled in the north are at a rate of 0.28 m per year. This is faster than the rate we found without the interaction effect, so we are seeing that not only are we drilling deeper wells faster over time, but that we are drilling deeper in northern regions than southern regions over time as well.\nTime Series Analysis - Decomposition\n\n\nShow code\n\n# created dataset to use as a timeseries object \nmonthly_depth <- region_north_south %>% \n  select(well_id, date_completed, depth_m, region_code, north_south, year_completed) %>% \n  mutate(month_completed = lubridate::month(date_completed)) %>% \n  mutate(month_year_completed = as.yearmon(paste0(year_completed, month_completed), \"%Y %m\")) %>% \n  mutate(month_year_completed = yearmonth(month_year_completed)) %>% \n  group_by(month_year_completed, north_south) %>% \n  mutate(month_mean_depth_m = mean(depth_m)) %>% # monthly averages\n  ungroup()\n\n\n\n\n\nShow code\n\n# classical decomposition analysis \nwells_depth_yearmon <- monthly_depth %>% \n  select(well_id, month_year_completed, depth_m) %>% \n  group_by(month_year_completed) %>% \n  summarize(mean_depth_m_year = mean(depth_m))\n\nwells_tbsl_2 <- as_tsibble(wells_depth_yearmon)\n\nwells_decomp_2 <- wells_tbsl_2 %>% \n  model(classical_decomposition(mean_depth_m_year, type = \"additive\")) %>%\n  components() %>% \n  autoplot()\n\nwells_decomp_2\n\n\n\n\nFigure 5: Decomposition of yearly mean well depth (m) is significant with trend component, but not with seasonal component\n\n\n\nInterpretation: We see there is evidence of a long-run upward trend over time and there is no evidence of seasonality. Based on the size of the bars, the trend component is more important in driving overall variation in yearly average well depth (m) since the gray bar for trend is much smaller than the bar for the seasonal component. This confirmed for me that the “cycles” I was seeing in Figure 6 were not significant. It also confirms what we already know: that over time we are drilling deeper wells in California.\nFuture Research\nFuture research could check to see if seasonality is significant in only northern regions. A closer look at Figure 6 shows that most of the seasonality is visible in regions one through five, which represent the northern region. This could make sense, as intuition tells me there could be a more seasonal affect in northern regions due to geology or colder climates.\nOther research could include policy as an independent variable in regression or other statistical analysis to see what kind of impact policy has on well drilling depth in California. This would be important to look at since we know that a large amount of wells are running dry in California4. And if groundwater levels continue to decline, we need to understand what policies are most effective so that we can better manage water now and for the future.\n\nAppendix\nFigures\n\n\nShow code\n\nwells_counts_year_region <- wells_region %>%\n  group_by(region_code, year_completed) %>%\n  count() %>%\n  rename(count = n)\n\ntot_wells_region <- ggplot(data = wells_counts_year_region, aes(x = year_completed, y = count, color = region_code)) +\n  geom_line() +\n  labs(x = \"Year well was completed\",\n       y = \"Number of wells\",\n       title = \"Total number of wells per year by region\",\n       colour = \"Region Code\") +\n  guides(color = guide_legend(nrow = 1)) +\n  theme_light() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 7),\n    legend.text = element_text(size = 7))\ntot_wells_region\n\n\n\n\nFigure 6: Total number of wells by region with potential for cyclical or seasonal trending.\n\n\n\nReferences\nFull code can be found at my GitHub repository hdolinh.github.io.\n\n\n\nJasechko, Scott, and Debra Perrone. 2020. “California’s Central Valley Groundwater Wells Run Dry During Recent Drought.” Earth’s Future 8. https://doi.org/10.1029/2019EF001339.\n\n\nJohnson, Tyler D., and Kenneth Belitz. 2015. “Identifying the Location and Population Served by Domestic Wells in California.” Journal of Hydrology: Regional Studies 3: 31–86. https://doi.org/10.1016/j.ejrh.2014.09.002.\n\n\nPerrone, Debra, and Scott Jasechko. 2019. “Deeper Well Drilling an Unsustainable Stopgap to Groundwater Depletion.” Nature Sustainability 2: 773–82. https://doi.org/10.1038/s41893-019-0325-z.\n\n\nJohnson and Belitz (2015)↩︎\nJasechko and Perrone (2020)↩︎\nPerrone and Jasechko (2019)↩︎\nJasechko and Perrone (2020)↩︎\n",
    "preview": "posts/2021-11-14-stats-final/stats-final_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-01-15T15:37:34-08:00",
    "input_file": "stats-final.knit.md",
    "preview_width": 1056,
    "preview_height": 672
  },
  {
    "path": "posts/2021-11-03-spatial-analysis-houston/",
    "title": "Spatial Analysis of Houston Power Outage",
    "description": "",
    "author": [
      {
        "name": "Halina Do-Linh",
        "url": {}
      }
    ],
    "date": "2021-11-03",
    "categories": [],
    "contents": "\nAssignment Objective\nIn February 2021, temperatures in Texas dipped dangerously low and resulted in a severe winter storms where 4.4 million people suffered extended outages of electricity and water.\nFor this assignment we used remotely-sensed night lights data from the Visible Infrared Imaging Radiometer Suite (VIIRS) to:\nEstimate how many residential buildings in Houston were without power on February 16, 2021.\nInvestigate how median income factored into the blackouts that occurred.\nPreparing the Data\nTo acquire the night lights image data we need to retrieve 4 files from NASA’s Level-1 and Atmosphere Archive & Distribution System Distributed Active Archive Center (LAADS DAAC).\nThese 4 files represent 2 days (2021/02/07 and 2021/02/16) of night lights image data. The reason we have 4 files to download is because the LAADS DAAC data are projected as a sinusoidal equal-area in a 36 x 18 grid and Houston is located on the border of the h08v05 and h08v06 tiles. To create a single image we need to download both tiles and then merge them.\nFirst we used the function read_dnb to download the Day Night Band (DNB) dataset from VNP46A1 granules.\n\n\nShow code\n\nread_dnb <- function(file_name) {\n  # Reads the \"DNB_At_Sensor_Radiance_500m\" dataset from a VNP46A1 granule into a STARS object.\n  # Then read the sinolsoidal tile x/y positions and adjust the STARS dimensions (extent+delta)\n\n  # The name of the dataset holding the nightlight band in the granule\n  dataset_name <- \"//HDFEOS/GRIDS/VNP_Grid_DNB/Data_Fields/DNB_At_Sensor_Radiance_500m\"\n\n  # From the metadata, we pull out a string containing the horizontal and vertical tile index\n  h_string <- gdal_metadata(file_name)[199]\n  v_string <- gdal_metadata(file_name)[219]\n  \n # str_split(h_string, \"=\", simplify = TRUE)\n  \n  # We parse the h/v string to pull out the integer number of h and v\n  tile_h <- as.integer(str_split(h_string, \"=\", simplify = TRUE)[[2]])\n  tile_v <- as.integer(str_split(v_string, \"=\", simplify = TRUE)[[2]])\n\n  # From the h/v tile grid position, we get the offset and the extent\n  west <- (10 * tile_h) - 180\n  north <- 90 - (10 * tile_v)\n  east <- west + 10\n  south <- north - 10\n\n  # A tile is 10 degrees and has 2400x2400 grid cells\n  delta <- 10 / 2400\n\n  # Reading the dataset\n  dnb <- read_stars(file_name, sub = dataset_name)\n\n  # Setting the CRS and applying offsets and deltas\n  st_crs(dnb) <- st_crs(4326)\n  st_dimensions(dnb)$x$delta <- delta\n  st_dimensions(dnb)$x$offset <- west\n  st_dimensions(dnb)$y$delta <- -delta\n  st_dimensions(dnb)$y$offset <- north\n  \n  return(dnb)\n}\n\n\n\nHere we used the read_dnb function to load in the 4 data sets as stars objects.\n\n\nShow code\n\nfeb07_05 <- here(\"_posts\", \"2021-11-03-spatial-analysis-houston\", \"data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v05.001.2021039064328.h5\")\nfeb07_05_dnb <- read_dnb(feb07_05)\n\nfeb16_05 <- here::here(\"_posts\", \"2021-11-03-spatial-analysis-houston\", \"data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v05.001.2021048091106.h5\")\nfeb16_05_dnb <- read_dnb(feb16_05)\n\nfeb07_06 <- here::here(\"_posts\", \"2021-11-03-spatial-analysis-houston\", \"data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v06.001.2021039064329.h5\")\nfeb07_06_dnb <- read_dnb(feb07_06)\n\nfeb16_06 <- here::here(\"_posts\", \"2021-11-03-spatial-analysis-houston\", \"data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v06.001.2021048091105.h5\")\nfeb16_06_dnb <- read_dnb(feb16_06)\n\n\n\nWe then combined the data sets into single stars objects using st_mosaic and then removed the dnb files from our environment using rm.\nWe additionally added our data folder to the .gitignore so that these data sets and all others moving forward would not be pushed to GitHub. This was a crucial step to implement because we had previously run into conflicts pushing our commits since the data sets were too large for GitHub’s file limit.\n\n\nShow code\n\nstars_feb07 <- st_mosaic(feb07_05_dnb, feb07_06_dnb)\n\nstars_feb16 <- st_mosaic(feb16_05_dnb, feb16_06_dnb)\n\nrm(feb07_05_dnb, feb16_05_dnb, feb07_06_dnb, feb16_06_dnb)\n\n\n\nAfter we created a blackout mask to represent the difference in night lights intensity caused by the storm. We did this by subtracting the 02/07 raster image from the 02/16 rage image. We also assumed that any location that experienced more than 200 nW cm-2 is a blackout location and so we assigned NA to those values. We then plotted the difference to see what it looks like.\n\n\nShow code\n\ndifference <- (stars_feb07 - stars_feb16) > 200\ndifference[difference == FALSE] <- NA\nplot(difference)\n\n\n\n\nWe vectorized the blackout mask using st_as_sf and then used st_make_valid to make the mask vector a valid geometry. After we plotted the vectorized mask we noticed the image is more contrasted, making it easier to see the difference.\n\n\nShow code\n\nmask_vector <- st_as_sf(difference)\nmask_vector <- st_make_valid(mask_vector)\nplot(mask_vector) #get a sense of what it looks like\n\n\n\n\nHowever, the vectorized blackout mask we created is not centered on our region of interest (ROI), which is Houston. So we created a bounding box (bbox) for the metropolitan Houston area and then used st_polygon to create a polygon.\nAfter we created the polygon, we converted it to a spatial feature and set the CRS to match the DNB data using st_sfc.\nBecause we want to spatially relate the Houston polygon to the blackout mask, we spatially subsetted the vectorized mask using R square brackets []. We then converted the CRS of the subsetted (or cropped) mask to NAD83/ Texas Centric Albers Equal Area (3083) using st_transform.\n\n\nShow code\n\n#points for bounding box\npt_1 <- st_point(c(-96.5, 29))\npt_2 <- st_point(c(-96.5, 30.5))\npt_3 <- st_point(c(-94.5, 30.5))\npt_4 <- st_point(c(-94.5, 29))\n\nhouston <- st_polygon(list(rbind(pt_1, pt_2, pt_3, pt_4, pt_1))) #repeat first coordinate to close box\n\nhouston_sf <- st_sfc(houston, crs = 4326)\n\nmask_crop <- mask_vector[houston_sf, op = st_intersects]\n\nmask_crop <- st_transform(x = mask_crop, crs = 3083)\nplot(mask_crop) #plotted to see if crop worked\n\n\n\n\nRoads Data\nWe want to ignore areas near highways since these areas account for a large amount of the night lights observable from space. We used highway spatial data from OpenStreetMap (OSM), but specifically from Geofabrik to retrieve a shapefile of all highways in Texas (as you can see in our query object).\nWe read in the roads data using st_read and then used st_transform to re-project the data from WGS 84 to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area).\nWe then made a 200 m buffer using st_buffer, but this creates undissolved buffers so we used st_union to dissolve them.\n\n\nShow code\n\nquery <- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\nhighways_84 <- sf::st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query)\n\nhighways <- sf::st_transform(highways_84, crs = 3083)\n\nhighway_buffer <- st_buffer(highways, dist = 200) %>% st_union()\nplot(highway_buffer) #plot to check\n\n\n\n\nBuildings Data\nWe need building data to look at which residential buildings were affected by the blackout. We obtained building data from OSM.\nWe queried the data to select only residential buildings, read in the data using st_read and then re-projected the data from WGS 84 to EPSG:3083 (NAD83/Texas Centric Albers Equal Area) using st_transform.\n\n\nShow code\n\nbuilding_query <- \"SELECT * \nFROM gis_osm_buildings_a_free_1 \nWHERE (type IS NULL AND name IS NULL) \nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nbuildings <- sf::st_read(\"data/gis_osm_buildings_a_free_1.gpkg\", query = building_query) %>% st_transform(crs = 3083)\n\n\n\nCensus Tract data\nWe want to investigate how median income factored into the blackouts that occurred. To do this we obtained socioeconomic data from the U.S. Census Bureau’s American Community Survey for Texas census tracts in 2019.\nWe used st_read to read in geodatabase layers for Texas and income, and then extracted the income data using R square brackets []. We then left-joined the Texas geodatabase data with the extracted income data using left_join. Lastly, we re-projected the joined data to EPSG:3083 (NAD83/Texas Centric Albers Equal Area) using st_transform.\n\n\nShow code\n\nacs_geoms <- sf::st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                     layer = \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\nacs_income <- sf::st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                      layer = \"X19_INCOME\")\n\n#extracted geoid and B19013e1 fields from income layer to do left_join\nmedian_income <- acs_income[c(\"GEOID\", \"B19013e1\")]\n\nacs_join <- left_join(acs_geoms, median_income, by = c(\"GEOID_Data\" = \"GEOID\"))\n\n#set new CRS to match Texas / NAD83\nacs_join <- st_transform(x = acs_join, crs = 3083)\n\n\n\nMerge the datasets\nBecause we want to ignore areas around highways, we need to remove the highway buffer from our vectorized blackout mask. We did this using st_difference.\nWe then spatially subsetted the data to find all the residential buildings in the blackout areas using R square brackets [].\nLastly, we spatially joined the building data with the joined ACS data using st_join.\n\n\nShow code\n\nrm_highways <- st_difference(mask_crop, highway_buffer)\n\nres_buildings_blackout <- buildings[rm_highways, op = st_intersects]\n\nbuilding_acs_join <- st_join(res_buildings_blackout, acs_join, st_intersects = \"GEOID\")\n\n\n\nQuestion 1: How many residential buildings were without power on 2021-02-16?\n\nThere were 157,411 residential buildings without power on 2021-02-16.\n\n\n\nShow code\n\nres_no_power <- buildings[rm_highways, op = st_intersects]\n\n\n\nSpatial Visualization Analysis\nTo tackle the bonus question, we wanted to create a map that shows the Houston blackout areas and the median income associated with those areas. To do this, we joined the rm_highways data frame (which contains DNB and geometry data) with the acs_join data frame (which contains income and geometry data). The result is the joined median_income_dnb data frame which contains median income data for just the blackout areas.\nWe created the map using tmap and we used the package rosm to plot a base map of Houston.\nBased on the map we made outlining the median income of residential areas that were affected by the blackout, the vast majority of impacted areas made $100,000 - $150,000 or less. There are only a few regions in central Houston that were both affected by the blackout and made over $250,000 in median income. This map shows that median income could be an important socioeconomic metric demonstrating which residents were more impacted by the power outage.\n\n\nShow code\n\nmedian_income_dnb <- st_join(rm_highways, acs_join, st_intersects = \"GEOID\")\n\nhouston_map <- osm.raster(st_bbox(houston)) #making open street map a raster with a bbox on houston\n\nblackout_income <- tm_shape(houston_map) +\n  tm_rgb(alpha = 0.75) +\n  tm_shape(rm_highways) + #adding the mask behind makes it show up better\n  tm_polygons() +\n  tm_shape(median_income_dnb) + #show Houston areas effected by the blackout\n  tm_fill(\"B19013e1\", n = 5, style = \"pretty\", title = \"Median Income\") + #fill by median income\n  tm_layout(title = \"Houston Blackout Areas by Median Income\",\n            legend.bg.color = \"lightgrey\",\n            legend.bg.alpha = 0.3)+\n  tm_scale_bar()+\n  tm_credits(\"Modified imagery from SNP VIIRS DNB satellite imagery on 2021-02-16. Base map taken \\nfrom OpenStreetMaps. Median income data drawn from the U.S. Census Bureau in 2019.\", align = \"right\", size = 0.6, bg.color = \"lightgrey\", bg.alpha = 0.3)\n\nblackout_income\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-11-03-spatial-analysis-houston/images/houston-blackout-income.png",
    "last_modified": "2022-01-15T15:30:48-08:00",
    "input_file": "spatial-analysis-houston.knit.md",
    "preview_width": 617,
    "preview_height": 465
  }
]
